
---

# **Lab 12 â€” Capstone Project: End-to-End GenAI Application**

### **ğŸš€ Build & Deploy a Production-Ready RAG Application From Scratch**

This capstone guides you to build **a complete, production-grade Generative AI application**, combining all concepts from Labs 1â€“11:

| Layer             | Component                                   |
| ----------------- | ------------------------------------------- |
| Data Layer        | Document ingestion, preprocessing, chunking |
| Embeddings        | Batch workers, async processing             |
| Retrieval         | Vector search, hybrid ranking               |
| LLM               | Prompting, templates, safety                |
| Application Layer | FastAPI server, task queue                  |
| Orchestration     | Worker pipelines, caching                   |
| Deployment        | Docker, orchestration, monitoring           |
| Scaling           | Autoscaling & load balancing                |
| Observability     | Logs, metrics, tracing                      |

You will deliver a **fully working system**, published via:

âœ” GitHub Repository
âœ” Docker Compose / K8s deployment
âœ” API documentation
âœ” README + architecture diagram
âœ” Demo notebook + sample queries

---

# ------------------------------------------

# **ğŸ¯ Capstone Goals**

# ------------------------------------------

By completing this project, you will:

* Build a complete production-grade **Retrieval-Augmented Generation (RAG)** pipeline
* Deploy a scalable **FastAPI + Worker + Weaviate** system
* Serve a working GenAI inference API
* Implement observability, security, and scaling components
* Produce a professional portfolio project suitable for job interviews

---

# ------------------------------------------

# **ğŸ§± Project Overview**

# ------------------------------------------

You will build:

```
GenAI Capstone System
â”‚
â”œâ”€â”€ 1) Document Ingestion Pipeline
â”œâ”€â”€ 2) Embedding & Storage Pipeline
â”œâ”€â”€ 3) RAG Query Engine
â”œâ”€â”€ 4) FastAPI Application Layer
â”œâ”€â”€ 5) Caching & Async Workers
â”œâ”€â”€ 6) Deployment (Docker Compose or Kubernetes)
â””â”€â”€ 7) Monitoring & Metrics
```

Your final deliverables:

* `notebooks/capstone_demo.ipynb`
* `backend/fastapi_app/`
* `backend/workers/`
* `vectorstore/weaviate/`
* `docker-compose.yaml` (or K8s manifests)
* Architecture diagram (PNG)
* README.md (Professional Documentation)

---

# ------------------------------------------

# **ğŸ“ Project Folder Structure**

# ------------------------------------------

```
capstone/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ fastapi_app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â””â”€â”€ shared/
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_docs/
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ weaviate/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ capstone_demo.ipynb
â”‚
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ README.md
```

---

# ------------------------------------------

# **1ï¸âƒ£ Step 1 â€” Document Ingestion Notebook**

# ------------------------------------------

### **Notebook: `capstone_demo.ipynb`**

```python
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter

docs_dir = "../data/raw_docs/"

documents = []
for file in os.listdir(docs_dir):
    if file.endswith(".pdf") or file.endswith(".txt"):
        with open(os.path.join(docs_dir, file), "rb") as f:
            text = f.read().decode("utf-8", errors="ignore")
            documents.append(text)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200,
)

chunks = splitter.split_text("\n".join(documents))
len(chunks)
```

---

# ------------------------------------------

# **2ï¸âƒ£ Step 2 â€” Embedding Pipeline (Workers)**

# ------------------------------------------

### `workers/tasks.py`

```python
from celery import Celery
from embeddings import embed_text
from vectorstore import store_vector

app = Celery("tasks", broker="redis://redis:6379/0")

@app.task
def embed_and_store(chunk, chunk_id):
    vector = embed_text(chunk)
    store_vector(chunk_id, vector)
    return {"id": chunk_id, "status": "stored"}
```

### Running workers

```bash
celery -A tasks worker --concurrency=4
```

---

# ------------------------------------------

# **3ï¸âƒ£ Step 3 â€” Vector DB Storage (Weaviate)**

# ------------------------------------------

### Example ingestion call

```python
from workers.tasks import embed_and_store

for i, chunk in enumerate(chunks):
    embed_and_store.delay(chunk, f"chunk-{i}")
```

---

# ------------------------------------------

# **4ï¸âƒ£ Step 4 â€” Build the RAG Engine**

# ------------------------------------------

### `services/rag.py`

```python
import weaviate
from llm import call_llm

client = weaviate.Client("http://vector-db:8080")

def rag_query(question):
    query_vector = embed(question)

    results = client.query.get("Document", ["text"]) \
        .with_near_vector({"vector": query_vector}) \
        .with_limit(5) \
        .do()

    context = "\n".join([r["text"] for r in results["data"]["Get"]["Document"]])

    prompt = f"""
    Use ONLY the context below to answer the question.

    Context:
    {context}

    Question: {question}
    """

    return call_llm(prompt)
```

---

# ------------------------------------------

# **5ï¸âƒ£ Step 5 â€” Expose the RAG API (FastAPI)**

# ------------------------------------------

### `fastapi_app/main.py`

```python
from fastapi import FastAPI
from services.rag import rag_query

app = FastAPI()

@app.post("/query")
def query(payload: dict):
    answer = rag_query(payload["question"])
    return {"answer": answer}
```

Run server:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

# ------------------------------------------

# **6ï¸âƒ£ Step 6 â€” Add Caching (Critical)**

# ------------------------------------------

### `utils/cache.py`

```python
from cachetools import TTLCache

cache = TTLCache(maxsize=1000, ttl=3600)

def cached_query(question, fn):
    if question in cache:
        return cache[question]

    result = fn(question)
    cache[question] = result
    return result
```

Integrate:

```python
@app.post("/query")
def query(payload):
    return cached_query(payload["question"], rag_query)
```

---

# ------------------------------------------

# **7ï¸âƒ£ Step 7 â€” Deploy With Docker Compose**

# ------------------------------------------

### **docker-compose.yaml**

```yaml
services:
  api:
    build: ./backend/fastapi_app
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - vector-db

  worker:
    build: ./backend/workers
    depends_on:
      - redis
      - vector-db
    deploy:
      replicas: 4

  redis:
    image: redis:7

  vector-db:
    image: semitechnologies/weaviate
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 20
      ENABLE_MODULES: text2vec-transformers
```

---

# ------------------------------------------

# **8ï¸âƒ£ Step 8 â€” Monitoring**

# ------------------------------------------

Add `/metrics`:

```python
from starlette_exporter import PrometheusMiddleware, handle_metrics

app.add_middleware(PrometheusMiddleware)
app.add_route("/metrics", handle_metrics)
```

Deploy **Prometheus + Grafana**.

---

# ------------------------------------------

# **9ï¸âƒ£ Step 9 â€” Scaling**

# ------------------------------------------

Scale workers:

```bash
docker compose up --scale worker=10
```

Scale FastAPI:

```bash
docker compose up --scale api=5
```

---

# ------------------------------------------

# **ğŸ”Ÿ Step 10 â€” Capstone Deliverables**

# ------------------------------------------

You must submit:

### âœ” **Working code repository**

### âœ” **Working Docker deployment**

### âœ” **API documentation (Swagger)**

### âœ” **Architecture diagram**

### âœ” **Demo notebook**

### âœ” **5 example RAG queries**

### âœ” **README + Setup Instructions**

---

# ------------------------------------------

# **ğŸ“ Final Capstone Exercises**

# ------------------------------------------

### **Exercise 1 â€” Add re-ranking**

Use Cohere or Jina reranker to improve answer quality.

### **Exercise 2 â€” Add embeddings batching**

Improve throughput.

### **Exercise 3 â€” Add OpenAI O1 or GPT-4o mini**

Include as an optional model.

### **Exercise 4 â€” Add metadata filters**

Filter retrieval by:

* date
* source
* topic

### **Exercise 5 â€” Add auth tokens to API**

Secure your app.

### **Exercise 6 â€” Deploy to cloud**

Choose:

* Fly.io
* AWS ECS
* GCP Cloud Run
* Modal

---

# ------------------------------------------

# ğŸ‰ **Congratulations â€” You completed the entire GenAI Engineering Capstone!**

# ------------------------------------------


